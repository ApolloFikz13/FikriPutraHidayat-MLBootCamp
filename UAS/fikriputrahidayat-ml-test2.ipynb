{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## DenseNet MNIST Dataset\nhttps://www.kaggle.com/szaitseff/under-the-hood-a-dense-net-w-mnist-dataset","metadata":{"_uuid":"65069ba70a42dec77013a5278dddeb1efff9df92"}},{"cell_type":"code","source":"# Load necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # data visualization\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical # to convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-08T06:20:00.797756Z","iopub.execute_input":"2022-07-08T06:20:00.798078Z","iopub.status.idle":"2022-07-08T06:20:01.857281Z","shell.execute_reply.started":"2022-07-08T06:20:00.798014Z","shell.execute_reply":"2022-07-08T06:20:01.856592Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1. MNIST DATA","metadata":{"_uuid":"dbf552bd663d81348eee7859bf5a54b3ef48162e","trusted":true}},{"cell_type":"code","source":"# Load datasets\ntrain, test = pd.read_csv(\"../input/train.csv\"), pd.read_csv(\"../input/test.csv\")","metadata":{"_uuid":"d7512437115d5f0e0f06de283dc0f34cb3e9107a","execution":{"iopub.status.busy":"2022-07-08T06:20:01.858530Z","iopub.execute_input":"2022-07-08T06:20:01.858975Z","iopub.status.idle":"2022-07-08T06:20:08.362740Z","shell.execute_reply.started":"2022-07-08T06:20:01.858935Z","shell.execute_reply":"2022-07-08T06:20:08.361980Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Review data\nprint(f'train data shape = {train.shape}', '/', f'test data shape = {test.shape}')\ntrain.head()","metadata":{"_uuid":"f60b8ede01f0ac1d54e80c9579849a3295354d92","execution":{"iopub.status.busy":"2022-07-08T06:20:08.363834Z","iopub.execute_input":"2022-07-08T06:20:08.364033Z","iopub.status.idle":"2022-07-08T06:20:08.498862Z","shell.execute_reply.started":"2022-07-08T06:20:08.364001Z","shell.execute_reply":"2022-07-08T06:20:08.498058Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# let's check the count of different labels in the dataset (~balanced)\ntrain['label'].value_counts()","metadata":{"_uuid":"68dd04b9c99fea496ff5d8bf555a8e491fab76ac","execution":{"iopub.status.busy":"2022-07-08T06:20:08.500279Z","iopub.execute_input":"2022-07-08T06:20:08.500533Z","iopub.status.idle":"2022-07-08T06:20:08.508663Z","shell.execute_reply.started":"2022-07-08T06:20:08.500487Z","shell.execute_reply":"2022-07-08T06:20:08.507778Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Numpy representation of the train and test data:\ntrain_pixels, test_pixels = train.iloc[:,1:].values.astype('float32'), test.values.astype('float32') # all pixel values\ntrain_labels = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits\ntrain_labels = train_labels.reshape(-1, 1) # ensure proper shape of the array\n\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')\nprint(f'train_labels shape = {train_labels.shape}')","metadata":{"_uuid":"6b06102607ec76c9516c2782823afbbe951788c5","execution":{"iopub.status.busy":"2022-07-08T06:20:08.509740Z","iopub.execute_input":"2022-07-08T06:20:08.509954Z","iopub.status.idle":"2022-07-08T06:20:08.724941Z","shell.execute_reply.started":"2022-07-08T06:20:08.509916Z","shell.execute_reply":"2022-07-08T06:20:08.723874Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Reshape input data to fit Keras model (height=28px, width=28px, channels=1):\ntrain_pixels, test_pixels = train_pixels.reshape(-1, 28, 28, 1), test_pixels.reshape(-1, 28, 28, 1)\nprint(f'train_pixels shape = {train_pixels.shape}')\nprint(f'test_pixels shape = {test_pixels.shape}')","metadata":{"_uuid":"960c6a5b844995fe12aa74d548893785d1d4382d","execution":{"iopub.status.busy":"2022-07-08T06:20:08.726542Z","iopub.execute_input":"2022-07-08T06:20:08.726894Z","iopub.status.idle":"2022-07-08T06:20:08.732997Z","shell.execute_reply.started":"2022-07-08T06:20:08.726824Z","shell.execute_reply":"2022-07-08T06:20:08.732065Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Visualize some images from the dataset:\nnrows, ncols = 3, 5  # number of rows and colums in subplots\nfig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True, figsize=(8,5))\nfor row in range(nrows):\n    for col in range(ncols):\n        i = np.random.randint(0, 30000)  # pick up arbitrary examples\n        ax[row, col].imshow(train_pixels[i,:,:,0], cmap='Greys')\n        ax[row, col].set_title(f'<{train.label[i]}>');","metadata":{"_uuid":"6caf10e515af9325cc2ed5715c65cdf53fc8f42a","scrolled":true,"execution":{"iopub.status.busy":"2022-07-08T06:20:08.734481Z","iopub.execute_input":"2022-07-08T06:20:08.734816Z","iopub.status.idle":"2022-07-08T06:20:10.059158Z","shell.execute_reply.started":"2022-07-08T06:20:08.734751Z","shell.execute_reply":"2022-07-08T06:20:10.057927Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Input data are greyscale pixels of intensity [0:255]. Let's normalize to [0:1]:\ntrain_pixels, test_pixels = train_pixels / 255.0, test_pixels / 255.0","metadata":{"_uuid":"56c89842676b38c6a589ebafec48748765c3dfcf","execution":{"iopub.status.busy":"2022-07-08T06:20:10.060954Z","iopub.execute_input":"2022-07-08T06:20:10.061518Z","iopub.status.idle":"2022-07-08T06:20:10.259630Z","shell.execute_reply.started":"2022-07-08T06:20:10.061249Z","shell.execute_reply":"2022-07-08T06:20:10.258814Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\ntrain_labels = to_categorical(train_labels, num_classes = 10)\nprint(f'train_labels shape = {train_labels.shape}')\ntrain_labels","metadata":{"_uuid":"b1b60ab298ae8fd9e7a5ed6564516c2a89e3733a","execution":{"iopub.status.busy":"2022-07-08T06:20:10.260749Z","iopub.execute_input":"2022-07-08T06:20:10.261268Z","iopub.status.idle":"2022-07-08T06:20:10.268552Z","shell.execute_reply.started":"2022-07-08T06:20:10.261169Z","shell.execute_reply":"2022-07-08T06:20:10.267685Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split training and validation set for the fitting\ntrain_pixels, val_pixels, train_labels, val_labels = train_test_split(train_pixels, train_labels, test_size = 0.1, random_state=2)\n\ntrain_pixels.shape, train_labels.shape, val_pixels.shape, val_labels.shape, test_pixels.shape","metadata":{"_uuid":"6cd9b63d7053c8415b08740d73f5f44cab6b2ca7","execution":{"iopub.status.busy":"2022-07-08T06:20:10.269620Z","iopub.execute_input":"2022-07-08T06:20:10.269898Z","iopub.status.idle":"2022-07-08T06:20:10.649105Z","shell.execute_reply.started":"2022-07-08T06:20:10.269858Z","shell.execute_reply":"2022-07-08T06:20:10.648301Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# let's fix the important numbers for further modeling:\nm_train = train_pixels.shape[0]   # number of examples in the training set\nm_val = val_pixels.shape[0]       # number of examples in the validation set\nm_test = test_pixels.shape[0]     # number of examples in the test set\nn_x = test.shape[1]               # input size, number of pixels in the image\nn_y = train_labels.shape[1]       # output size, number of label classes\nprint(f\" m_train = {m_train} / m_val = {m_val} / m_test = {m_test} / n_x = {n_x} / n_y = {n_y}\")","metadata":{"_uuid":"c5169f7d244c26c8200b08efe656c2a437cb096a","execution":{"iopub.status.busy":"2022-07-08T06:20:10.650424Z","iopub.execute_input":"2022-07-08T06:20:10.650657Z","iopub.status.idle":"2022-07-08T06:20:10.656760Z","shell.execute_reply.started":"2022-07-08T06:20:10.650615Z","shell.execute_reply":"2022-07-08T06:20:10.655836Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 2. A DENSE NEURAL NETWORK IN KERAS","metadata":{"_uuid":"01a1775d75c696511bc0c49c3c6f8471a9a8da4f"}},{"cell_type":"code","source":"# Decide on the model architecture: [n_x, hidden_layers, n_y]\nlayer_dims = [n_x, 512, n_y]  # the model architecture is adjustable","metadata":{"_uuid":"c7f3917252f075f8ffe60f7c43ce4dede1d5bf9b","execution":{"iopub.status.busy":"2022-07-08T06:20:10.657895Z","iopub.execute_input":"2022-07-08T06:20:10.658116Z","iopub.status.idle":"2022-07-08T06:20:10.668000Z","shell.execute_reply.started":"2022-07-08T06:20:10.658075Z","shell.execute_reply":"2022-07-08T06:20:10.667190Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# create an instance of a neural network:\nk_model = Sequential()\n# the first hidden layer must have input dimensions:\nk_model.add(Flatten(input_shape=[28,28,1]))\nk_model.add(Dense(layer_dims[1], activation='relu',\n                  kernel_regularizer=l2(0)))\nk_model.add(Dropout(0.25))\n# additional hidden layers are optional\n# output layer w/softmax activation:\nk_model.add(Dense(n_y, activation='softmax',\n                  kernel_regularizer=l2(0)))\n\n# Compile the model w/Adam optimizer:\nk_model.compile(optimizer=Adam(lr=1e-3),\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss', \n                             patience=1, verbose=0, \n                             factor=0.5, min_lr=1e-8)\n# Train the model:\nk_model.fit(train_pixels, train_labels, epochs=20, batch_size=128,\n            callbacks=[lr_decay], verbose=0)\n\n# Evaluate the model:\nk_train_loss, k_train_acc = k_model.evaluate(train_pixels, train_labels)\nk_val_loss, k_val_acc = k_model.evaluate(val_pixels, val_labels)\n\nprint(f'k_model: train accuracy = {round(k_train_acc * 100, 4)}%')\nprint(f'k_model: val accuracy = {round(k_val_acc * 100, 4)}%')\nprint(f'k_model: val error = {round((1 - k_val_acc) * m_val)} examples')\n","metadata":{"_uuid":"dc83c21b61ea3b543d1b6f018e0f2095ec7f3f3a","execution":{"iopub.status.busy":"2022-07-08T06:20:10.668912Z","iopub.execute_input":"2022-07-08T06:20:10.669232Z","iopub.status.idle":"2022-07-08T06:21:06.646079Z","shell.execute_reply.started":"2022-07-08T06:20:10.669180Z","shell.execute_reply":"2022-07-08T06:21:06.644768Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## 3. A CUSTOM NEURAL NETWORK IN PYTHON/NUMPY (classification)","metadata":{"_uuid":"26043c058ef21042c6904b13f67eae090b1dde54"}},{"cell_type":"markdown","source":"Let's have a look \"under the hood\" at a similar model in Python/Numpy and appreciate the greatness of Keras :)","metadata":{"_uuid":"f97ea283c78102ddf6cce0ee8acc0695e40585d4","trusted":true}},{"cell_type":"markdown","source":"### 3.1 Reshape data and define mini-batches","metadata":{"_uuid":"03a534e0682b05f855ba4f6f7daf64199c04e07e"}},{"cell_type":"code","source":"# Reshape data to fit the Custom Model architecture: (n_pixels, m_examples)\nX_train = train_pixels.reshape(m_train, -1).T\nY_train = train_labels.T\nX_val   = val_pixels.reshape(m_val,-1).T\nY_val   = val_labels.T\nX_test  = test_pixels.reshape(m_test, -1).T\n\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape","metadata":{"_uuid":"bfec56b49c612bead60b5ea36fab2bba84cf84d7","execution":{"iopub.status.busy":"2022-07-08T06:21:06.647677Z","iopub.execute_input":"2022-07-08T06:21:06.647966Z","iopub.status.idle":"2022-07-08T06:21:06.655351Z","shell.execute_reply.started":"2022-07-08T06:21:06.647868Z","shell.execute_reply":"2022-07-08T06:21:06.654479Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define a function to create random mini-batches for the gradient descent:\ndef random_mini_batches(X, Y, batch_size):\n    \"\"\"\n    This funcion creates a list of random minibatches from (X, Y)\n    Arguments:\n        X -- input data, of shape (input size, number of examples)\n        Y -- \"true\" labels vector, of shape (output size, number of examples)\n        batch_size -- size of mini-batches, integer\n    Returns:\n        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    m = X.shape[1]              # number of examples\n    mini_batches = []           # initialize a list to contain all minibatches\n        \n    # Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n\n    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = m // batch_size # number of minibatches of size batch_size\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*batch_size:(k+1)*batch_size]\n        mini_batch_Y = shuffled_Y[:, k*batch_size:(k+1)*batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","metadata":{"_uuid":"a33497c7eb7b156483f9f19580a76a28bc430819","execution":{"iopub.status.busy":"2022-07-08T06:21:06.656925Z","iopub.execute_input":"2022-07-08T06:21:06.657215Z","iopub.status.idle":"2022-07-08T06:21:06.666625Z","shell.execute_reply.started":"2022-07-08T06:21:06.657124Z","shell.execute_reply":"2022-07-08T06:21:06.665398Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Define a neural network model with fully connected layers","metadata":{"_uuid":"a78c676ec752b9fe400a4965a4e5f55f3bcf5de7"}},{"cell_type":"code","source":"class Custom_model(object):\n    \n    def __init__(self, layer_dims):\n        \"\"\" \n        The model consists of the input layer (pixels), a number of hidden layers and\n        the output layer (categorical classifier). To create an instance of the model, we set\n        dimensions of its layers and initialize parameters for the hidden/ output layers.\n        Arguments: \n            layer_dims -- list containing the input size and each layer size\n        Returns: \n            parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1) \n        \"\"\"\n        self.layer_dims = layer_dims       # a list with dimensions of all layers\n        self.num_layers = len(layer_dims)  # number of layers (with input layer)\n        self.parameters = {}        # a dictionary with weights and biases of the model\n        # Initializing weights randomly (He initialization) and biases to zeros\n        for l in range(1, len(layer_dims)):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], \n                                                       layer_dims[l-1])*np.sqrt(2./layer_dims[l-1])\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n    \n    # define getters and setters for accessing the model class attributes:        \n    def get_layer_dims(self):\n        return self.layer_dims\n    def get_num_layers(self):\n        return self.num_layers\n    def get_params(self, key):\n        return self.parameters.get(key)\n    def set_params(self, key, value):\n        self.parameters[key] = value\n    \n        \n    def forward_propagation(self, X, keep_prob):\n        \"\"\" \n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n        Arguments:\n            X -- data, numpy array of shape (input size, number of examples)\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns:\n            AL -- last post-activation value\n            caches -- list of caches containing:\n               every cache of layers w/ReLU activation (there are L-1 of them, indexed from 0 to L-2)\n               the cache of the output layer with Softmax activation (there is one, indexed L-1)\n        \"\"\"\n        caches = []\n        L = self.get_num_layers()-1    # number of layers with weights (hidden + output)\n        A = X                          # set input as the first hidden layer activation\n        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n        for l in range(1, L):\n            A_prev = A                # initialize activation of the previous layer\n            W, b = self.get_params(f'W{l}'), self.get_params(f'b{l}') # get weights and biases\n            Z = W.dot(A_prev) + b     # linear activation for the hidden layers\n            A = np.maximum(0,Z)       # ReLU activation for the hidden layers\n            if keep_prob == 1:        # if no dropout\n                cache = (A_prev, Z)   # useful during backpropagation\n            elif keep_prob < 1:       # if dropout is used for regularization\n                D = np.random.rand(A.shape[0], A.shape[1])  # initialize matrix D\n                D = D < keep_prob   # convert entries of D to 0/1 (using keep_prob as threshold)\n                A *= D              # shut down some neurons of A\n                A /= keep_prob      # scale the value of neurons that haven't been shutdown\n                cache = (A_prev, Z, D)   # useful during backpropagation\n            caches.append(cache)\n        # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n        W, b = self.get_params(f'W{L}'), self.get_params(f'b{L}')\n        Z = W.dot(A) + b                        # Linear activation of the output layer\n        Z -= np.max(Z, axis=0, keepdims=True)   # Normalize Z to make Softmax stable\n        AL = np.exp(Z)/np.sum(np.exp(Z),axis=0,\n                              keepdims=True) # Softmax activation of the output layer\n        cache = (A, Z)                          # useful during backpropagation\n        caches.append(cache)\n        return AL, caches\n        \n    \n    def compute_cost(self, AL, Y, lambd):\n        \"\"\"\n        Implement the cost function with L2 regularization.\n        Arguments:\n            AL -- post-activation, output of forward propagation,\n                                    of shape (output size, number of examples)\n            Y -- \"true\" labels vector, of shape (output size, number of examples)\n            lambd -- regularization hyperparameter, scalar\n        Returns:\n            cost - value of the regularized loss function\n        \"\"\"\n        m = AL.shape[1]               # number of training examples\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        assert(Y.shape == AL.shape)\n        # Compute the cross-entropy part of the cost for Softmax activation function:\n        cross_entropy_cost = -(1./m) * np.sum(np.multiply(Y, np.log(AL)))\n\n        # Compute L2 regularization cost\n        L2_reg_cost = 0\n        if lambd == 0:\n            pass\n        else:\n            for l in range(1, L+1):     # sum of all squared weights\n                L2_reg_cost += (1./m)*(lambd/2)*(np.sum(np.square(self.get_params(f'W{l}'))))\n\n        # Total cost:\n        cost = cross_entropy_cost + L2_reg_cost\n        # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n        cost = np.squeeze(cost)   \n        return cost\n    \n            \n    def backward_propagation(self, AL, Y, caches, lambd, keep_prob):\n        \"\"\" \n        Implement the backward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SOFTMAX]\n        Arguments:\n            AL -- probability vectors for the training examples, output of the forward propagation\n            Y -- true one-hot \"label\" vectors for the training examples\n            caches -- list of caches containing:\n                every cache of forward propagation with \"relu\" \n                                        (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of forward propagation with \"softmax\" (there is one, index L-1)\n            lambd -- lambda, an L2 regularization parameter, scalar\n            keep_prob - probability of keeping a neuron active during drop-out, scalar\n        Returns: grads -- a dictionary with updated gradients\n        \"\"\"\n        L = self.get_num_layers()-1  # number of layers with weights (hidden and output)\n        m = AL.shape[1]              # number of training examples\n        assert(Y.shape == AL.shape)\n        grads = {}                   # a dict for the gradients of the cost function\n\n        # Lth layer (SOFTMAX -> LINEAR) gradients.\n        W = self.get_params(f\"W{L}\") # get weights for the output layer\n        A_prev, Z = caches[L-1]  # get inputs and linear activations for the output layer\n        dZ = AL - Y                  # Gradient of the cost w.r.t. Z (from calculus)\n        dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n        db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n        dA_prev = np.dot(W.T,dZ)                        # Gradient of cost w.r.t. dA_prev\n        # Update the grads dictionary for the output layer\n        grads[f\"dW{L}\"], grads[f\"db{L}\"] = dW, db\n\n        # l-th hidden layer: (RELU -> LINEAR) gradients.\n        for l in reversed(range(1, L)):\n            W = self.get_params(f\"W{l}\") # get weights for the l-th hidden layer\n            dA = dA_prev\n            if keep_prob == 1:           # without dropout\n                A_prev, Z = caches[l-1]  # get ReLU & linear activations for l-th hidden layer\n            elif keep_prob < 1:          # with dropout\n                A_prev, Z, D = caches[l-1]  # get ReLU & linear activations + mask\n                dA *= D                     # apply mask to shut down the same neurons as in f.p.\n                dA /= keep_prob             # scale the value of the remaining neurons\n            dZ = np.array(dA, copy=True)    # just converting dz to a correct object.\n            dZ[Z <= 0] = 0                  # when z <= 0, set dz to 0 as well. \n            dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W  # Gradient of cost w.r.t. W\n            db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n            dA_prev = np.dot(W.T,dZ)          # Gradient of cost w.r.t. dA_prev\n            # Update the grads dictionary for the hidden layers\n            grads[f\"dA{l}\"] = dA\n            grads[f\"dW{l}\"], grads[f\"db{l}\"] = dW, db\n        return grads\n    \n        \n    def update_parameters_with_gd(self, grads, lr):\n        \"\"\" \n        Update parameters of the model using method gradient descent\n        Arguments:\n            grads -- python dictionary containing gradients, output of backprop\n            lr -- the learning rate, scalar\n        Returns: \n            updated parameters (weithts and biases of the model)\n        \"\"\"\n        L = self.get_num_layers()-1  # get number of layers with weights (hidden + output)\n        for l in range(1, L+1):\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr * grads[f\"dW{l}\"])\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr * grads[f\"db{l}\"])\n               \n            \n    def initialize_adam(self):\n        \"\"\"\n        Initializes v and s for the Adam optimizer as two python dictionaries with:\n            keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            values: numpy arrays of zeros of the same shape as the corresponding gradients.\n        Returns: \n            v -- python dict that will contain the exponentially weighted average of the gradient.\n            s -- python dict that will contain the exponentially weighted average of the squared gradient.\n        \"\"\"\n        v = {}   \n        s = {}\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden and output)\n        for l in range(1, L+1):\n            v[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            v[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))\n            s[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            s[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))    \n        return v,s\n    \n    \n    def update_parameters_with_adam(self, grads, v, s, t, lr, beta1, beta2, epsilon):\n        \"\"\"\n        Update parameters of the model using Adam optimization algorithm\n        Arguments:\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n            t -- current timestep (minibatch)\n            lr -- the learning rate, scalar.\n            beta1 -- Exponential decay hyperparameter for the first moment estimates \n            beta2 -- Exponential decay hyperparameter for the second moment estimates \n            epsilon -- hyperparameter preventing division by zero in Adam updates\n        Returns:\n            updated parameters (model attributes)\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n        \"\"\"\n        v_corr = {}       # Initializing a bias-corrected first moment\n        s_corr = {}       # Initializing a bias corrected second moment estimate\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden+output)\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L+1):\n            # Moving average of the gradients\n            v[f\"dW{l}\"] = beta1*v[f\"dW{l}\"] + (1-beta1)*grads[f\"dW{l}\"]\n            v[f\"db{l}\"] = beta1*v[f\"db{l}\"] + (1-beta1)*grads[f\"db{l}\"]\n\n            # Compute bias-corrected first moment estimate\n            v_corr[f\"dW{l}\"] = v[f\"dW{l}\"]/(1-beta1**t)\n            v_corr[f\"db{l}\"] = v[f\"db{l}\"]/(1-beta1**t)\n\n            # Moving average of the squared gradients\n            s[f\"dW{l}\"] = beta2*s[f\"dW{l}\"] + (1-beta2)*grads[f\"dW{l}\"]**2\n            s[f\"db{l}\"] = beta2*s[f\"db{l}\"] + (1-beta2)*grads[f\"db{l}\"]**2\n\n            # Compute bias-corrected second raw moment estimate\n            s_corr[f\"dW{l}\"] = s[f\"dW{l}\"]/(1-beta2**t)\n            s_corr[f\"db{l}\"] = s[f\"db{l}\"]/(1-beta2**t)\n\n            # Update parameters\n            temp_W = v_corr[f\"dW{l}\"]/(s_corr[f\"dW{l}\"]**0.5 + epsilon)\n            temp_b = v_corr[f\"db{l}\"]/(s_corr[f\"db{l}\"]**0.5 + epsilon)\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr*temp_W)\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr*temp_b)\n\n        return v, s\n        \n        \n    def predict(self, X):\n        \"\"\" \n        This method is used to predict the results of a L-layer neural network.\n        Arguments:\n            X -- dataset of examples to label, shape (num_pixels, num_examples)\n        Returns:\n            p -- predictions of one-hot labels for X, shape (num_classes, num_examples)\n        \"\"\"\n        m = X.shape[1]          # number of examples\n        # Forward propagation\n        AL, _ = self.forward_propagation(X, keep_prob=1)\n        n = AL.shape[0]         # number of classes\n        p = np.zeros((n, m))    # initialize predictions\n        # convert probabilities AL to one-hot label predictions:\n        for i in range(m):\n            max_i = np.amax(AL[:,i], axis=0)\n            for j in range(n):\n                if AL[j,i] == max_i:\n                    p[j,i] = 1\n                else:\n                    p[j,i] = 0\n        return np.int64(p)\n    \n        \n    def fit(self, X, Y, batch_size=128, num_epochs=20, lr=1e-3, min_lr=1e-8, lambd=0, keep_prob=0.75,\n            beta1=0.9, beta2=0.999, epsilon=1e-8, optimizer='adam', print_cost=False):\n        \"\"\" \n        Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n        Arguments:\n            X -- data, numpy array of shape (# pixels, # examples)\n            Y -- true \"label\" vector, of shape (# classes, # examples)\n            batch_size -- the size of a mini batch on which parameters are get updated\n            num_epochs -- number of epochs, i.e. passes through the training set\n            lr -- learning rate of the gradient descent update rule\n            min_lr -- the lower threshold of the learning rate decay\n            lambd -- lambda, the L2 regularization hyperparameter\n            optimizer -- optimization metod [\"gd\"=gradient_descent or \"adam\"]\n            beta1 -- exp decay hyperparameter for the past gradients estimates in 'adam'\n            beta2 -- exp decay hyperparameter for the past squared gradients estimates in 'adam'\n            epsilon -- hyperparameter preventing division by zero in 'adam' updates\n            print_cost -- if True, it prints the cost every # steps\n        Returns:\n            parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []                # to keep track of the cost\n        lr_0 = lr                 # to fix the initial learning rate before decay\n        t = 0                     # initializing the minibatch counter (for Adam update)\n        cost_prev = 1000          # initialize cost to a big number\n        # Initialize the optimizer\n        if optimizer == \"gd\":\n            pass                  # no initialization required for gradient descent\n        elif optimizer == \"adam\":\n            v, s = self.initialize_adam()\n        # Optimization loop\n        for epoch in range(num_epochs):\n            # Define the random minibatches. Reshuffle the dataset after each epoch\n            minibatches = random_mini_batches(X, Y, batch_size)\n            # Initialize a list of minibatch costs for each epoch:\n            minibatch_costs = []\n            # update parameters after each minibatch:\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Forward propagation: [LINEAR->RELU]*(L-1) -> LINEAR->SOFTMAX.\n                AL, caches = self.forward_propagation(minibatch_X, keep_prob)\n                # Compute cost.\n                cost = self.compute_cost(AL, minibatch_Y, lambd)\n                minibatch_costs.append(cost)\n                # Backward propagation.\n                grads = self.backward_propagation(AL, minibatch_Y, caches, lambd, keep_prob)\n                # Update parameters\n                t = t + 1 # minibatch counter\n                if optimizer == \"gd\":\n                    self.update_parameters_with_gd(grads, lr)\n                elif optimizer == \"adam\":\n                    v, s = self.update_parameters_with_adam(grads, v, s, t, lr, \n                                                            beta1, beta2, epsilon)\n            # Find average cost over all minibathes within the epoch: \n            ave_cost = sum(minibatch_costs) / len(minibatch_costs)\n            # Define learning rate decay:\n            if ave_cost > cost_prev and lr > min_lr:\n                lr = lr / 2   # reduce lr, but not below the min value\n            cost_prev = ave_cost  # save cost value for the next epoch\n                \n            # Print the cost every 2 epoch\n            if print_cost and epoch % 2 == 0:\n                print(f\"Cost after epoch {epoch}: {ave_cost}\")\n            if print_cost and epoch % 1 == 0:    \n                costs.append(ave_cost)\n        # plot the Learning curve\n        if print_cost:\n            plt.plot(np.squeeze(costs))\n            plt.ylabel(\"cost\")\n            plt.xlabel(\"epochs (x 1)\")\n            plt.title(f\"Learning rate = {lr_0} / {lr}\")\n            plt.show()","metadata":{"_uuid":"d6c44b43902f8d7225ebc28f151f9c749fb2e97e","execution":{"iopub.status.busy":"2022-07-08T06:21:06.668364Z","iopub.execute_input":"2022-07-08T06:21:06.668665Z","iopub.status.idle":"2022-07-08T06:21:06.711588Z","shell.execute_reply.started":"2022-07-08T06:21:06.668612Z","shell.execute_reply":"2022-07-08T06:21:06.710741Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## 4. MODEL TRAINING","metadata":{"_uuid":"83a4cde7a4f46ae7a3ac924bdc584a434e425c9c"}},{"cell_type":"code","source":"# Create list to save journal records of the current session\nrecords_list = []","metadata":{"_uuid":"8d8743a90ed9f9bf593386bdc41c338987cb2491","execution":{"iopub.status.busy":"2022-07-08T06:21:06.713643Z","iopub.execute_input":"2022-07-08T06:21:06.714190Z","iopub.status.idle":"2022-07-08T06:21:06.726681Z","shell.execute_reply.started":"2022-07-08T06:21:06.713940Z","shell.execute_reply":"2022-07-08T06:21:06.725806Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the Custom_model class with 'layer_dims' architecture:\nc_model = Custom_model(layer_dims)\n\n# Set hyperparameters that we want to tune for the custom model:\nlr          = 1e-3   # the learning rate for the gradient descent\nmin_lr      = 1e-8   # the lower threshold of the learning rate decay\noptimizer   = 'adam'\nbatch_size  = 128  \nnum_epochs  = 25          \nlambd       = 0   # lambda - regularization hyperparameter, scalar\nkeep_prob   = 0.75   # keep_prob - probability of keeping a neuron active during dropout\n\n# Train the model at various hyperparameters settings:\nc_model.fit(X_train, Y_train, batch_size=batch_size, num_epochs=num_epochs,\n            lr=lr, min_lr=min_lr, lambd=lambd, keep_prob=keep_prob,\n            optimizer=optimizer, print_cost=True)       \n\n# Evaluation on the train data:\npredict_train = c_model.predict(X_train)\ncorrect_train = np.argmax(predict_train, axis=0) == np.argmax(Y_train, axis=0)\nc_train_acc = round(np.sum(correct_train)/m_train, 2)\n\n# Evaluation on the validation data:\npredict_val = c_model.predict(X_val)\ncorrect_val = np.argmax(predict_val, axis=0) == np.argmax(Y_val, axis=0)\nc_val_acc = round(np.sum(correct_val)/m_val, 2)\n\nprint(f\"c_model: train accuracy: {c_train_acc * 100}%\")\nprint(f\"c_model: val accuracy = {c_val_acc * 100}%\")\nprint(f'c_model: val error = {round((1 - c_val_acc) * m_val)} examples')\n\n# Update the journal of hyperparameters tuning records\nrecord = {'layer_dims'   : c_model.get_layer_dims(), \n          'acc_train'    : c_train_acc, \n          'acc_val'      : c_val_acc,\n          'val_error'    : round((1 - c_val_acc) * m_val),\n          'batch_size'   : batch_size,\n          'num_epochs'   : num_epochs,\n          'lr'           : lr,\n          'min_lr'       : min_lr,\n          'lambda'       : lambd,\n          'keep_prob'    : keep_prob}\n\nrecords_list.append(record)\njournal = pd.DataFrame(records_list)   # saves records when repeatedly running the current cell\njournal","metadata":{"_uuid":"1831854f8d5da69d4f7351b42df457e17dd13cb3","execution":{"iopub.status.busy":"2022-07-08T06:21:06.728354Z","iopub.execute_input":"2022-07-08T06:21:06.728735Z","iopub.status.idle":"2022-07-08T06:24:28.335768Z","shell.execute_reply.started":"2022-07-08T06:21:06.728663Z","shell.execute_reply":"2022-07-08T06:24:28.334741Z"},"trusted":true},"execution_count":18,"outputs":[]}]}
